{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week0_09 practice: PyTorch practice, hints and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits:\n",
    "* First part is based on YSDA [Practical RL course week04 materials](https://github.com/yandexdataschool/Practical_RL/tree/master/week04_%5Brecap%5D_deep_learning).\n",
    "* Second part is based on PyTorch official tutorials and [this kaggle kernel](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader)\n",
    "* Third part is based on PyTorch tutorial by [Stanford CS 231n course](http://cs231n.stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://pytorch.org/tutorials/_static/pytorch-logo-dark.svg)\n",
    "\n",
    "__This notebook__ will remind you how to use pytorch low and high-level core. You can install it [here](http://pytorch.org/).\n",
    "\n",
    "__Pytorch feels__ differently than other frameworks (like tensorflow/theano) on almost every level. TensorFlow makes your code live in two \"worlds\" simultaneously:  symbolic graphs and actual tensors. First you declare a symbolic \"recipe\" of how to get from inputs to outputs, then feed it with actual minibatches of data.  In pytorch, __there's only one world__: all tensors have a numeric value.\n",
    "\n",
    "You compute outputs on the fly without pre-declaring anything. The code looks exactly as in pure numpy with one exception: pytorch computes gradients for you. And can run stuff on GPU. And has a number of pre-implemented building blocks for your neural nets. [And a few more things.](https://medium.com/towards-data-science/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:59:42.103233Z",
     "start_time": "2020-02-28T00:59:42.099338Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Tensormancy\n",
    "\n",
    "__1.1 The [_disclaimer_](https://gist.githubusercontent.com/justheuristic/e2c1fa28ca02670cabc42cacf3902796/raw/fd3d935cef63a01b85ed2790b5c11c370245cbd7/stddisclaimer.h)__\n",
    "\n",
    "Let's write another function, this time in polar coordinates:\n",
    "$$\\rho(\\theta) = (1 + 0.9 \\cdot cos (6 \\cdot \\theta) ) \\cdot (1 + 0.01 \\cdot cos(24 \\cdot \\theta)) \\cdot (0.5 + 0.05 \\cdot cos(200 \\cdot \\theta)) \\cdot (10 + sin(10 \\cdot \\theta))$$\n",
    "\n",
    "\n",
    "Then convert it into cartesian coordinates ([howto](http://www.mathsisfun.com/polar-cartesian-coordinates.html)) and plot the results.\n",
    "\n",
    "Use torch tensors only: no lists, loops, numpy arrays, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:20.890514Z",
     "start_time": "2020-02-28T00:13:20.639022Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = torch.linspace(-np.pi, np.pi, steps=1000)\n",
    "\n",
    "# compute rho(theta) as per formula above\n",
    "rho = ### YOUR CODE HERE\n",
    "\n",
    "# Now convert polar (rho, theta) pairs into cartesian (x,y) to plot them.\n",
    "x = ### YOUR CODE HERE\n",
    "y = ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.fill(x.numpy(), y.numpy(), color='red')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Using the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:20.896055Z",
     "start_time": "2020-02-28T00:13:20.893241Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:21.223097Z",
     "start_time": "2020-02-28T00:13:21.220643Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/basic_s20/week0_09_Optimization_and_Regularization_in_DL/notmnist.py -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:24.681345Z",
     "start_time": "2020-02-28T00:13:21.715775Z"
    }
   },
   "outputs": [],
   "source": [
    "from notmnist import load_notmnist\n",
    "X_train, y_train, X_test, y_test = load_notmnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T08:04:08.525398Z",
     "start_time": "2020-02-28T08:04:08.519240Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetMNIST(Dataset):\n",
    "    def __init__(self, path='./notMNIST_small', letters='ABCDEFGHIJ', transform=None):\n",
    "        self.data, self.labels, _ ,_  = load_notmnist(path=path, letters=letters, test_size=0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        image = self.data[index].transpose(1, 2, 0)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T08:04:19.890914Z",
     "start_time": "2020-02-28T08:04:19.336026Z"
    }
   },
   "outputs": [],
   "source": [
    "full_dataset = DatasetMNIST('./notMNIST_small', 'AB', transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T08:05:36.271686Z",
     "start_time": "2020-02-28T08:05:36.267914Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can access and get data with index by __getitem__(index)\n",
    "img, lab = full_dataset.__getitem__(0)\n",
    "\n",
    "print(img.shape)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T08:05:39.406582Z",
     "start_time": "2020-02-28T08:05:39.402397Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torchvision.transforms.ToTensor()\n",
    "\n",
    "a(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:37.378205Z",
     "start_time": "2020-02-28T00:13:37.115219Z"
    }
   },
   "outputs": [],
   "source": [
    "inds = np.random.randint(len(full_dataset), size=2)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(full_dataset[inds[i]][0].reshape([28,28]))\n",
    "    plt.title(str(full_dataset[inds[i]][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:38.561437Z",
     "start_time": "2020-02-28T00:13:38.558371Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(full_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use dataloader as iterator by using iter() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:39.386457Z",
     "start_time": "2020-02-28T00:13:39.382386Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at images and labels of batch size by extracting data `.next()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:40.508098Z",
     "start_time": "2020-02-28T00:13:40.502629Z"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = train_iter.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:41.593728Z",
     "start_time": "2020-02-28T00:13:41.506364Z"
    }
   },
   "outputs": [],
   "source": [
    "# make grid takes tensor as arg\n",
    "# tensor : (batchsize, channels, height, width)\n",
    "grid = torchvision.utils.make_grid(images.permute([0, 3, 1, 2]))\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:48.795066Z",
     "start_time": "2020-02-28T00:13:46.451771Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_with_transform = DatasetMNIST(\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:48.801336Z",
     "start_time": "2020-02-28T00:13:48.797410Z"
    }
   },
   "outputs": [],
   "source": [
    "img, lab = train_dataset_with_transform.__getitem__(0)\n",
    "\n",
    "print('image shape at the first row : {}'.format(img.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:52.064233Z",
     "start_time": "2020-02-28T00:13:52.057623Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader_tr = DataLoader(train_dataset_with_transform, batch_size=8, shuffle=True)\n",
    "\n",
    "train_iter_tr = iter(train_loader_tr)\n",
    "print(type(train_iter_tr))\n",
    "\n",
    "images, labels = train_iter_tr.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:53.200621Z",
     "start_time": "2020-02-28T00:13:53.062965Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(labels.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing several transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to take data augmentation, you have to make List using `torchvision.transforms.Compose`\n",
    "\n",
    "```\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "```\n",
    "\n",
    "\n",
    "this function can convert some image by order within `__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:54.766219Z",
     "start_time": "2020-02-28T00:13:54.762711Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __call__(self, pic):\n",
    "        return pic.flatten()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:55.430127Z",
     "start_time": "2020-02-28T00:13:55.427338Z"
    }
   },
   "outputs": [],
   "source": [
    "a = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:56.147330Z",
     "start_time": "2020-02-28T00:13:56.143060Z"
    }
   },
   "outputs": [],
   "source": [
    "a(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:13:56.918185Z",
     "start_time": "2020-02-28T00:13:56.915283Z"
    }
   },
   "outputs": [],
   "source": [
    "new_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    Flatten()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:37:37.781950Z",
     "start_time": "2020-02-28T09:37:37.779388Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T00:56:29.075866Z",
     "start_time": "2020-02-28T00:56:29.071564Z"
    }
   },
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:28:18.459203Z",
     "start_time": "2020-02-28T09:28:18.455997Z"
    }
   },
   "outputs": [],
   "source": [
    "def subset_ind(dataset, ratio: float):\n",
    "#     return ### YOUR CODE HERE\n",
    "    return np.random.choice(len(dataset), size=int(ratio*len(dataset)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:32:54.001780Z",
     "start_time": "2020-02-28T09:32:51.635588Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DatasetMNIST(\n",
    "    './notMNIST_small',\n",
    "#     'AB',\n",
    "    transform=new_transform\n",
    ")\n",
    "\n",
    "shrink_inds = subset_ind(dataset, 0.2)\n",
    "dataset = Subset(dataset, shrink_inds)\n",
    "\n",
    "print(f'\\n\\n dataset size: {len(dataset)}, labels: {np.unique(dataset.dataset.labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:32:54.028315Z",
     "start_time": "2020-02-28T09:32:54.004282Z"
    }
   },
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "val_inds = subset_ind(dataset, val_size)\n",
    "\n",
    "train_dataset = Subset(dataset, [i for i in range(len(dataset)) if i not in val_inds])\n",
    "val_dataset = Subset(dataset, val_inds)\n",
    "\n",
    "print(f'  training size: {len(train_dataset)}\\nvalidation size: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:32:59.926520Z",
     "start_time": "2020-02-28T09:32:59.922784Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:00.954231Z",
     "start_time": "2020-02-28T09:33:00.947835Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))\n",
    "\n",
    "images, labels = train_iter.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:35:42.679368Z",
     "start_time": "2020-02-28T09:35:42.676594Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:35:44.204284Z",
     "start_time": "2020-02-28T09:35:44.199944Z"
    }
   },
   "outputs": [],
   "source": [
    "# create network again just in case\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 10),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "model.to(device, torch.float32)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:02.897035Z",
     "start_time": "2020-02-28T09:33:02.884404Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, opt, n_epochs: int):\n",
    "    '''\n",
    "    model: нейросеть для обучения,\n",
    "    train_loader, val_loader: загрузчики данных\n",
    "    loss_fn: целевая метрика (которую будем оптимизировать)\n",
    "    opt: оптимизатор (обновляет веса нейросети)\n",
    "    n_epochs: кол-во эпох, полных проходов датасета\n",
    "    '''\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        ep_train_loss = []\n",
    "        ep_val_loss = []\n",
    "        ep_val_accuracy = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True) # enable dropout / batch_norm training behavior\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # move data to target device\n",
    "            ### YOUR CODE HERE\n",
    "\n",
    "            # train on batch: compute loss, calc grads, perform optimizer step and zero the grads\n",
    "            ### YOUR CODE HERE\n",
    "            ep_train_loss.append(loss.item())\n",
    "\n",
    "        model.train(False) # disable dropout / use averages for batch_norm\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                # move data to target device\n",
    "                ### YOUR CODE HERE\n",
    "\n",
    "                # compute predictions\n",
    "                ### YOUR CODE HERE\n",
    "                ep_val_loss.append(### YOUR CODE HERE)\n",
    "                y_pred = ### YOUR CODE HERE\n",
    "                ep_val_accuracy.append(### YOUR CODE HERE)\n",
    "\n",
    "        # print the results for this epoch:\n",
    "        print(f'Epoch {epoch + 1} of {n_epochs} took {time.time() - start_time:.3f}s')\n",
    "\n",
    "        train_loss.append(np.mean(ep_train_loss))\n",
    "        val_loss.append(np.mean(ep_val_loss))\n",
    "        val_accuracy.append(np.mean(ep_val_accuracy))\n",
    "        \n",
    "        print(f\"\\t  training loss: {train_loss[-1]:.6f}\")\n",
    "        print(f\"\\tvalidation loss: {val_loss[-1]:.6f}\")\n",
    "        print(f\"\\tvalidation accuracy: {val_accuracy[-1]:.3f}\")\n",
    "\n",
    "    return train_loss, val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:33.252494Z",
     "start_time": "2020-02-28T09:33:28.712014Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "train_loss, val_loss, val_accuracy = train_model(model, train_loader, val_loader, loss_func, opt, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:35.242213Z",
     "start_time": "2020-02-28T09:33:35.237045Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_train_process(train_loss, val_loss, val_accuracy):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].plot(train_loss, label='train')\n",
    "    axes[0].plot(val_loss, label='validation')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].set_title('Validation accuracy')\n",
    "    axes[1].plot(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:33:36.057391Z",
     "start_time": "2020-02-28T09:33:35.611228Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_process(train_loss, val_loss, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:44:50.420339Z",
     "start_time": "2020-02-28T09:44:50.406364Z"
    }
   },
   "outputs": [],
   "source": [
    "# create network again just in case\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "model.to(device, torch.float32)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:45:08.145156Z",
     "start_time": "2020-02-28T09:44:52.924801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "train_loss, val_loss, val_accuracy = train_model(model, train_loader, val_loader, loss_func, opt, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:45:13.396284Z",
     "start_time": "2020-02-28T09:45:13.032994Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_process(train_loss, val_loss, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "Try to add some additional transformations (e.g. random crop, rotation etc.) and train your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchnorm try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save the model (model checkpointing)\n",
    "\n",
    "Now we have trained a model! Obviously we do not want to retrain the model everytime we want to use it. Plus if you are training a super big model, you probably want to save checkpoint periodically so that you can always fall back to the last checkpoint in case something bad happened or you simply want to test models at different training iterations.\n",
    "\n",
    "Model checkpointing is fairly simple in PyTorch. First, we define a helper function that can save a model to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    # state_dict: a Python dictionary object that:\n",
    "    # - for a model, maps each layer to its parameter tensor;\n",
    "    # - for an optimizer, contains info about the optimizer’s states and hyperparameters used.\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a brand new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Testing -- you should get a pretty poor performance since the model hasn't learned anything yet.\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a training loop with model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save(epoch, save_interval, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            # different from before: saving model checkpoints\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)\n",
    "            iteration += 1\n",
    "        test()\n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save(5, save_interval=500, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load from the final checkpoint\n",
    "load_checkpoint('mnist-4690.pth', model, optimizer)\n",
    "# should give you the final model accuracy\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "### More about pytorch:\n",
    "* Using torch on GPU and multi-GPU - [link](http://pytorch.org/docs/master/notes/cuda.html)\n",
    "* More tutorials on pytorch - [link](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* Pytorch examples - a repo that implements many cool DL models in pytorch - [link](https://github.com/pytorch/examples)\n",
    "* Practical pytorch - a repo that implements some... other cool DL models... yes, in pytorch - [link](https://github.com/spro/practical-pytorch)\n",
    "* And some more - [link](https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [1, 0, 1, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts_as_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts_as_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_projected = np.random.randint(0, 10, (15, 50, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_element = np.arange(len(verts_projected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.arange(3), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-4a08077ffe54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_verts_projected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverts_projected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverts_projected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "new_verts_projected = np.concatenate([verts_projected, np.arange(verts_projected.shape[0])[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.reshape(verts_projected, (-1, 2))\n",
    "b = np.repeat(np.arange(verts_projected.shape[0]), verts_projected.shape[1])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 2)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 1)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_projected = np.random.randint(0, 10, (15, 50, 2))\n",
    "new_verts_projected = np.hstack([\n",
    "    np.repeat(np.arange(verts_projected.shape[0]), verts_projected.shape[1])[:, None],\n",
    "    np.reshape(verts_projected, (-1, 2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  9,  4],\n",
       "       [ 0,  1,  7],\n",
       "       [ 0,  0,  6],\n",
       "       ...,\n",
       "       [14,  9,  9],\n",
       "       [14,  7,  7],\n",
       "       [14,  9,  9]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_verts_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 2)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_verts_projected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.randint(0, 2, (15, 1, 10, 10)).astype(bool)\n",
    "mask[..., -1]\n",
    "\n",
    "verts_projected = np.random.randint(0, 10, (15, 50, 2))\n",
    "new_verts_projected = np.hstack([\n",
    "    np.repeat(np.arange(verts_projected.shape[0]), verts_projected.shape[1])[:, None],\n",
    "    np.reshape(verts_projected, (-1, 2))\n",
    "])\n",
    "\n",
    "verts_as_matrix = np.zeros_like(mask[:, -1, :, :])\n",
    "verts_as_matrix[verts_projected[:, 0], verts_projected[:, 1], verts_projected[:, 2]] = True\n",
    "c = verts_as_matrix * ~mask[:, -1, :, :]\n",
    "\n",
    "indices = np.where(c)\n",
    "\n",
    "\n",
    "images1 = np.random.randint(0, 256, (15, 3, 10, 10))\n",
    "images2 = np.random.randint(0, 256, (15, 3, 10, 10))\n",
    "\n",
    "images2[indices, :] = images1[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(torch.arange(10), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.from_numpy(mask).type(torch.BoolTensor)\n",
    "images1 = torch.from_numpy(images1)\n",
    "images2 = torch.from_numpy(images2)\n",
    "verts_projected = torch.from_numpy(verts_projected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.randint(0, 2, (15, 1, 10, 10)).astype(bool)\n",
    "mask[..., -1]\n",
    "\n",
    "verts_projected = np.random.randint(0, 10, (15, 50, 2))\n",
    "new_verts_projected = np.hstack([\n",
    "    np.repeat(np.arange(verts_projected.shape[0]), verts_projected.shape[1])[:, None],\n",
    "    np.reshape(verts_projected, (-1, 2))\n",
    "])\n",
    "\n",
    "verts_as_matrix = np.zeros_like(mask[:, -1, :, :])\n",
    "verts_as_matrix[new_verts_projected[:, 0], new_verts_projected[:, 1], new_verts_projected[:, 2]] = True\n",
    "c = verts_as_matrix * ~mask[:, -1, :, :]\n",
    "\n",
    "indices = np.where(c)\n",
    "\n",
    "\n",
    "images1 = np.random.randint(0, 256, (15, 3, 10, 10))\n",
    "images2 = np.random.randint(0, 256, (15, 3, 10, 10))\n",
    "\n",
    "images2[indices, :] = images1[indices, :]\n",
    "\n",
    "\n",
    "mask = torch.from_numpy(mask).type(torch.BoolTensor)\n",
    "images1 = torch.from_numpy(images1)\n",
    "images2 = torch.from_numpy(images2)\n",
    "verts_projected = torch.from_numpy(verts_projected)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_verts_projected = torch.cat(\n",
    "    [\n",
    "        torch.repeat_interleave(torch.arange(verts_projected.shape[0]), verts_projected.shape[1])[:, None],\n",
    "        verts_projected.view((-1, 2))\n",
    "    ],\n",
    "    dim=1\n",
    ")\n",
    "verts_as_matrix = torch.zeros_like(mask[:, -1, :, :])\n",
    "verts_as_matrix[new_verts_projected[:, 0], new_verts_projected[:, 1], new_verts_projected[:, 2]] = True\n",
    "c = verts_as_matrix * ~mask[:, -1, :, :]\n",
    "\n",
    "indices = torch.where(c)\n",
    "\n",
    "images2[indices[0], :, indices[1], indices[2]] = images1[indices[0], :, indices[1], indices[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 10, 10])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 10, 10])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts_as_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "         7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "         9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "        11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
       "        13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14]),\n",
       " array([0, 1, 1, 1, 2, 3, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 9, 0, 2, 2,\n",
       "        3, 3, 3, 4, 4, 4, 4, 6, 7, 7, 8, 8, 9, 9, 0, 0, 0, 1, 1, 1, 1, 2,\n",
       "        3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 9, 1, 1, 2, 2, 2, 2,\n",
       "        3, 3, 4, 4, 5, 6, 6, 8, 8, 8, 8, 9, 9, 9, 0, 0, 0, 1, 1, 2, 2, 2,\n",
       "        2, 3, 3, 4, 4, 4, 4, 4, 5, 6, 6, 7, 7, 7, 8, 8, 8, 8, 9, 0, 1, 1,\n",
       "        1, 1, 1, 1, 2, 2, 2, 3, 4, 4, 4, 5, 5, 6, 7, 8, 9, 0, 2, 3, 3, 5,\n",
       "        5, 5, 6, 6, 7, 7, 7, 9, 9, 0, 0, 1, 1, 2, 2, 4, 4, 4, 4, 6, 6, 7,\n",
       "        8, 9, 9, 0, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 7, 8, 9, 9,\n",
       "        0, 0, 0, 2, 2, 2, 3, 3, 3, 4, 6, 6, 6, 7, 8, 9, 9, 9, 9, 0, 0, 1,\n",
       "        2, 2, 2, 3, 3, 3, 4, 4, 6, 6, 7, 7, 8, 9, 9, 1, 1, 2, 3, 4, 5, 6,\n",
       "        6, 6, 6, 7, 8, 8, 8, 9, 9, 9, 0, 0, 1, 1, 1, 2, 3, 4, 5, 5, 5, 6,\n",
       "        7, 7, 7, 8, 9, 9, 9, 0, 0, 1, 1, 2, 2, 3, 4, 4, 5, 6, 8, 9, 0, 0,\n",
       "        1, 1, 1, 1, 2, 3, 3, 3, 4, 5, 5, 6, 6, 6, 6, 7, 7, 9]),\n",
       " array([0, 3, 6, 7, 0, 1, 5, 1, 3, 8, 9, 2, 7, 9, 2, 4, 6, 9, 0, 6, 7, 8,\n",
       "        5, 6, 9, 3, 4, 5, 7, 0, 6, 9, 3, 4, 3, 5, 2, 5, 9, 1, 3, 6, 8, 3,\n",
       "        4, 0, 2, 5, 6, 0, 7, 8, 2, 3, 8, 4, 7, 8, 5, 9, 1, 2, 2, 6, 7, 8,\n",
       "        7, 9, 1, 9, 7, 3, 4, 0, 1, 4, 9, 1, 2, 7, 1, 5, 6, 3, 7, 4, 5, 7,\n",
       "        8, 4, 7, 3, 4, 5, 6, 9, 6, 5, 8, 1, 3, 8, 0, 1, 8, 9, 6, 6, 0, 2,\n",
       "        3, 7, 8, 9, 0, 1, 7, 2, 2, 6, 7, 4, 7, 5, 2, 3, 8, 9, 7, 8, 9, 4,\n",
       "        7, 8, 1, 7, 2, 3, 6, 0, 3, 1, 7, 0, 1, 0, 8, 0, 1, 2, 7, 0, 8, 7,\n",
       "        3, 0, 8, 1, 0, 1, 3, 7, 4, 7, 0, 3, 5, 5, 9, 0, 3, 4, 9, 0, 6, 9,\n",
       "        4, 5, 6, 0, 6, 9, 0, 8, 9, 4, 0, 5, 6, 7, 9, 1, 3, 4, 6, 1, 2, 8,\n",
       "        0, 1, 2, 2, 3, 6, 3, 6, 4, 8, 7, 8, 2, 1, 5, 0, 9, 4, 0, 0, 9, 0,\n",
       "        4, 8, 9, 6, 3, 4, 5, 0, 1, 8, 0, 9, 1, 2, 3, 5, 8, 7, 0, 2, 7, 7,\n",
       "        2, 3, 9, 3, 4, 5, 9, 2, 6, 2, 5, 0, 2, 7, 3, 6, 9, 1, 8, 6, 2, 3,\n",
       "        2, 5, 6, 7, 3, 1, 4, 7, 5, 1, 8, 3, 4, 6, 8, 0, 2, 7]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(verts_as_matrix * ~mask[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False, False, False,  True, False,  True,  True],\n",
       "        [ True,  True, False, False,  True, False, False, False,  True, False],\n",
       "        [False,  True,  True, False,  True,  True,  True,  True, False,  True],\n",
       "        [ True, False,  True,  True,  True, False,  True, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False,  True,  True,  True, False],\n",
       "        [False, False, False, False, False, False,  True, False, False, False],\n",
       "        [False,  True, False,  True,  True,  True,  True, False, False, False],\n",
       "        [ True, False, False,  True, False, False, False,  True,  True,  True],\n",
       "        [ True,  True, False, False,  True,  True, False,  True,  True, False],\n",
       "        [False,  True,  True,  True,  True,  True, False, False, False, False]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,\n",
       "          5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "          5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13,\n",
       "         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "         14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 1, 1, 1, 2, 3, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 9, 0, 2, 2, 3, 3,\n",
       "         3, 4, 4, 4, 4, 6, 7, 7, 8, 8, 9, 9, 0, 0, 0, 1, 1, 1, 1, 2, 3, 4, 4, 4,\n",
       "         4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 9, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 5, 6,\n",
       "         6, 8, 8, 8, 8, 9, 9, 9, 0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4,\n",
       "         5, 6, 6, 7, 7, 7, 8, 8, 8, 8, 9, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 4, 4,\n",
       "         4, 5, 5, 6, 7, 8, 9, 0, 2, 3, 3, 5, 5, 5, 6, 6, 7, 7, 7, 9, 9, 0, 0, 1,\n",
       "         1, 2, 2, 4, 4, 4, 4, 6, 6, 7, 8, 9, 9, 0, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5,\n",
       "         5, 6, 6, 6, 7, 8, 9, 9, 0, 0, 0, 2, 2, 2, 3, 3, 3, 4, 6, 6, 6, 7, 8, 9,\n",
       "         9, 9, 9, 0, 0, 1, 2, 2, 2, 3, 3, 3, 4, 4, 6, 6, 7, 7, 8, 9, 9, 1, 1, 2,\n",
       "         3, 4, 5, 6, 6, 6, 6, 7, 8, 8, 8, 9, 9, 9, 0, 0, 1, 1, 1, 2, 3, 4, 5, 5,\n",
       "         5, 6, 7, 7, 7, 8, 9, 9, 9, 0, 0, 1, 1, 2, 2, 3, 4, 4, 5, 6, 8, 9, 0, 0,\n",
       "         1, 1, 1, 1, 2, 3, 3, 3, 4, 5, 5, 6, 6, 6, 6, 7, 7, 9]),\n",
       " tensor([0, 3, 6, 7, 0, 1, 5, 1, 3, 8, 9, 2, 7, 9, 2, 4, 6, 9, 0, 6, 7, 8, 5, 6,\n",
       "         9, 3, 4, 5, 7, 0, 6, 9, 3, 4, 3, 5, 2, 5, 9, 1, 3, 6, 8, 3, 4, 0, 2, 5,\n",
       "         6, 0, 7, 8, 2, 3, 8, 4, 7, 8, 5, 9, 1, 2, 2, 6, 7, 8, 7, 9, 1, 9, 7, 3,\n",
       "         4, 0, 1, 4, 9, 1, 2, 7, 1, 5, 6, 3, 7, 4, 5, 7, 8, 4, 7, 3, 4, 5, 6, 9,\n",
       "         6, 5, 8, 1, 3, 8, 0, 1, 8, 9, 6, 6, 0, 2, 3, 7, 8, 9, 0, 1, 7, 2, 2, 6,\n",
       "         7, 4, 7, 5, 2, 3, 8, 9, 7, 8, 9, 4, 7, 8, 1, 7, 2, 3, 6, 0, 3, 1, 7, 0,\n",
       "         1, 0, 8, 0, 1, 2, 7, 0, 8, 7, 3, 0, 8, 1, 0, 1, 3, 7, 4, 7, 0, 3, 5, 5,\n",
       "         9, 0, 3, 4, 9, 0, 6, 9, 4, 5, 6, 0, 6, 9, 0, 8, 9, 4, 0, 5, 6, 7, 9, 1,\n",
       "         3, 4, 6, 1, 2, 8, 0, 1, 2, 2, 3, 6, 3, 6, 4, 8, 7, 8, 2, 1, 5, 0, 9, 4,\n",
       "         0, 0, 9, 0, 4, 8, 9, 6, 3, 4, 5, 0, 1, 8, 0, 9, 1, 2, 3, 5, 8, 7, 0, 2,\n",
       "         7, 7, 2, 3, 9, 3, 4, 5, 9, 2, 6, 2, 5, 0, 2, 7, 3, 6, 9, 1, 8, 6, 2, 3,\n",
       "         2, 5, 6, 7, 3, 1, 4, 7, 5, 1, 8, 3, 4, 6, 8, 0, 2, 7]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where((verts_as_matrix[:, None, :, :] * ~mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(images2[indices[0], :, indices[1], indices[2]], images1[indices[0], :, indices[1], indices[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 50, 2])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts_projected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-1560d0b148b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mverts_projected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "xs = []\n",
    "ys = []\n",
    "for x, y in verts_projected[i]:\n",
    "    if mask[i, int(x), int(y)] == False:\n",
    "        xs.append(x)\n",
    "        ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1, 2, 2, 3, 4, 4, 4, 5, 6, 7, 8, 8, 9]),\n",
       " tensor([0, 5, 6, 3, 9, 0, 1, 4, 5, 8, 9, 6, 1, 7, 2]),\n",
       " tensor([2, 4, 6, 4, 6, 7, 6, 6, 0, 0, 2, 5, 3, 5, 0]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffa4613ae90>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images1[0] - images2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 2, 3, 3, 3, 5, 5, 6, 7, 7, 7, 8, 8, 8, 9, 9]),\n",
       " array([3, 4, 7, 3, 1, 4, 8, 2, 3, 6, 0, 1, 5, 1, 4, 6, 0, 7]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(1, 4)', '(2, 5)', '(3, 6)']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[''.join(str(x)) for x in zip([1,2,3], [4,5,6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([1,2,3], [4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-3e63e34346c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
     ]
    }
   ],
   "source": [
    "''.join(zip([1,2,3], [4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def numUniqueEmails(self, emails) -> int:\n",
    "        splitted = [_email.split('@') for _email in emails]\n",
    "        print(splitted)\n",
    "        basenames = [x[0] for x in splitted]\n",
    "        basenames = [x.split('+')[0].replace('.', '') for x in basenames]\n",
    "        print(basenames)\n",
    "        print(list(zip(basenames, [x[1] for x in splitted])))\n",
    "        final_list = set(['@'.join(x) for x in zip(basenames, [x[1] for x in splitted])])\n",
    "        return final_list, len(set(final_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = Solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['test.email+alex', 'leetcode.com'], ['test.email', 'leetcode.com']]\n",
      "['testemail', 'testemail']\n",
      "[('testemail', 'leetcode.com'), ('testemail', 'leetcode.com')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'testemail@leetcode.com'}, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution.numUniqueEmails([\"test.email+alex@leetcode.com\", \"test.email@leetcode.com\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict()\n",
    "a.setdefault(1, set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.setdefault(1, set()).add(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {3}}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.setdefault(1, set()).add(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {3}}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.setdefault(1, set()).add(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {3, 5}}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def ladderLength(self, beginWord: str, endWord: str, wordList: List[str]) -> int:\n",
    "        wordLen = len(wordList[0])\n",
    "        pr_dict = {}\n",
    "        inv_pr_dict = {}\n",
    "        for word in wordList:\n",
    "            for i in range(wordLen):\n",
    "                local_word = list(word)\n",
    "                local_word[i] = '*'\n",
    "                pr_dict.setdefault(''.join(local_word), set()).add(word)\n",
    "        return pr_dict\n",
    "    \n",
    "        queue = [(beginWord, 1)]\n",
    "        visited_dict = {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
