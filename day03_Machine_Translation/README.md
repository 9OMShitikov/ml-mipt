Neural Machine Translation as seq2sec:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/harbour_dlia_s21/day03_Machine_Translation/practice_seq2seq_NMT_from_class.ipynb)


Further readings:

* Great blog post by Jay Alammar: https://jalammar.github.io/illustrated-transformer/
* Notebook on positional encoding: [link](https://github.com/ml-mipt/ml-mipt/blob/advanced/week04_Transformer/week04_positional_encoding_carriers.ipynb)
* Great Annotated Transformer article with code and comments by Harvard NLP group: https://nlp.seas.harvard.edu/2018/04/03/attention.html
